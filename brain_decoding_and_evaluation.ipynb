{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer\n",
    "\n",
    "from peft import PeftConfig, PeftModel, LoraConfig, TaskType\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict, GridSearchCV\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all load the PEFT models and get the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, DebertaV2ForMaskedLM\n",
    "from transformers import AutoModelForQuestionAnswering, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "from transformers import DebertaV2ForQuestionAnswering, DebertaV2ForSequenceClassification, DebertaV2ForTokenClassification\n",
    "\n",
    "# BASELINE MODELS\n",
    "\n",
    "# repo_name = \"bert-base-uncased\"\n",
    "# repo_name = \"roberta-base\"\n",
    "# repo_name = \"microsoft/deberta-base\"\n",
    "# repo_name = \"microsoft/deberta-v3-base\"\n",
    "# repo_name = \"google/electra-base-generator\"\n",
    "# repo_name = \"facebook/bart-base\"\n",
    "# repo_name = \"gpt2\"\n",
    "\n",
    "\n",
    "# TESTING THE MODELS. MODELS ARE OPTIMIZED WITH PEFT LIBRARY, USING LORA WITH CAUSAL LM AND RANK=8\n",
    "\n",
    "# repo_name = \"alitolga/bert-base-uncased-large-peft\"\n",
    "# repo_name = \"alitolga/gpt2-large-peft\"\n",
    "# repo_name = \"alitolga/bart-base-large-peft\"\n",
    "# repo_name = \"alitolga/roberta-base-large-peft\"\n",
    "# repo_name = \"alitolga/deberta-base-large-peft\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-large-peft\"\n",
    "repo_name = \"alitolga/electra-base-generator-large-peft\"\n",
    "\n",
    "\n",
    "# MODELS FOR TESTING THE RANK\n",
    "\n",
    "# repo_name = \"alitolga/deberta-v3-base-RealRank1\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank2\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank4\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank8\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank16\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank32\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-rank64\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-Rank1\"\n",
    "\n",
    "# repo_name = \"alitolga/electra-base-generator-Rank1\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank2\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank4\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank8\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank16\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank32\"\n",
    "# repo_name = \"alitolga/electra-base-generator-rank64\"\n",
    "# repo_name = \"alitolga/electra-base-generator-Rank128\"\n",
    "\n",
    "# MODELS FOR TESTING THE TASK TYPE\n",
    "\n",
    "# repo_name = \"alitolga/deberta-v3-base-FeatureExtraction\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-QuestionAns\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-SeqClassification\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-Wnut-TokenClassification\"\n",
    "\n",
    "# repo_name = \"alitolga/electra-base-generator-FeatureExtraction\"\n",
    "# repo_name = \"alitolga/electra-base-generator-QuestionAns\"\n",
    "# repo_name = \"alitolga/electra-base-generator-SeqClassification\"\n",
    "# repo_name = \"alitolga/electra-base-generator-Wnut-TokenClassification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f378fb62050c41cfa965ddc79bc1b71c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 33,838,906 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "config = PeftConfig.from_pretrained(repo_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = AutoModelForQuestionAnswering.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = AutoModelForTokenClassification.from_pretrained(config.base_model_name_or_path, num_labels=13)\n",
    "\n",
    "\n",
    "# base_model = DebertaV2ForMaskedLM.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = DebertaV2ForQuestionAnswering.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = DebertaV2ForSequenceClassification.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = DebertaV2ForTokenClassification.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, repo_name, config=config)\n",
    "model = PeftModel.from_pretrained(base_model, repo_name)\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"helena-balabin/pereira_fMRI_sentences\"\n",
    "\n",
    "# Specify the path to save or load the dataset\n",
    "save_path = \"./data\"\n",
    "\n",
    "# Load the dataset, use the cache if available\n",
    "dataset = load_dataset(dataset_name, cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "sentences = dataset[\"train\"][\"sentences\"]\n",
    "sentences_zero = sentences[0] # 0th subject\n",
    "print(len(sentences_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra cell to use SentEval library for evaluating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import absolute_import, division\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# # Set PATHs\n",
    "# PATH_TO_SENTEVAL = 'SentEval/'\n",
    "# PATH_TO_DATA = 'SentEval/data'\n",
    "\n",
    "# # import SentEval\n",
    "# sys.path.insert(0, PATH_TO_SENTEVAL)\n",
    "# import senteval\n",
    "\n",
    "\n",
    "# # SentEval prepare and batcher\n",
    "# def prepare(params, samples):\n",
    "#     return\n",
    "\n",
    "# def batcher(params, batch):\n",
    "#     se_sentences = [sent if sent != [] else ['.'] for sent in batch]\n",
    "#     sentences_str = [' '.join(sentence) for sentence in se_sentences]\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.pad_token = tokenizer.eos_token\n",
    "#     inputs = tokenizer(sentences_str, return_tensors='pt', truncation=True, padding=True)\n",
    "#     outputs = model(**inputs, output_hidden_states=True)\n",
    "#     hidden_states = outputs.hidden_states\n",
    "#     embeddings = torch.mean(hidden_states[0], dim=1)\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "\n",
    "# # Set params for SentEval\n",
    "# params_senteval = {'task_path': PATH_TO_DATA, 'usepytorch': True, 'kfold': 5}\n",
    "# params_senteval['classifier'] = {'nhid': 0, 'optim': 'rmsprop', 'batch_size': 128,\n",
    "#                                  'tenacity': 3, 'epoch_size': 2}\n",
    "\n",
    "# # Set up logger\n",
    "# logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.DEBUG)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     se = senteval.engine.SE(params_senteval, batcher, prepare)\n",
    "#     transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16',\n",
    "#                       'MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC',\n",
    "#                       'SICKEntailment', 'SICKRelatedness', 'STSBenchmark',\n",
    "#                       'Length', 'WordContent', 'Depth', 'TopConstituents',\n",
    "#                       'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',\n",
    "#                       'OddManOut', 'CoordinationInversion']\n",
    "#     results = se.eval(transfer_tasks)\n",
    "#     print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the sentence embeddings from the Peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    embeddings = torch.mean(hidden_states[0], dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(sentences_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 256)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embeddings.detach().numpy()\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example fold indices: [240 369 260 169 245 217 378 129 308 374 287  38  67   8 116 247 140   9\n",
      " 218 112  10  66 265 242  82  93  96 107  37 251  26  94 161 191 234 194\n",
      "  43 317 178 244 357 149 371 281  48 286 285 207 214 259  33 348  52 147\n",
      " 352  75 209 329 202 155 223 341  32 159 319  65 268 200 365 238 382 360\n",
      " 243  72 133 300 162]\n",
      "Number of sentences in the example fold: 77\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Assuming Y is your (384, embedding_size) array of sentence embeddings\n",
    "# embedding_size = 100  # Example embedding size\n",
    "\n",
    "# Calculate the mean embedding across all sentences\n",
    "mean_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "# Calculate Euclidean distances of each embedding to the mean embedding\n",
    "distances = euclidean_distances(embeddings, mean_embedding.reshape(1, -1)).flatten()\n",
    "\n",
    "# Sort embeddings by their distance to the mean embedding\n",
    "sorted_indices = np.argsort(distances)\n",
    "\n",
    "# Calculate the number of sentences per batch for 5 equal-sized folds\n",
    "batch_size = len(embeddings) // 5\n",
    "\n",
    "# Distribute sorted indices into 5 batches\n",
    "batches = [sorted_indices[i::5] for i in range(5)]\n",
    "\n",
    "# Each batch now acts as a fold, and every fold contains sentences from all ranges of distances\n",
    "fold_indices = batches  # For clarity, as each batch is essentially a fold\n",
    "\n",
    "# Verify one of the folds\n",
    "print(f\"Example fold indices: {fold_indices[0]}\")\n",
    "print(f\"Number of sentences in the example fold: {len(fold_indices[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD]'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the Brain Decoding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 195127)\n"
     ]
    }
   ],
   "source": [
    "# Get the voxels. For simplicity we start with all the brain regions\n",
    "fmri_data = dataset[\"train\"][\"all\"]\n",
    "\n",
    "# fMRI data of the first subject out of 8\n",
    "subject_zero = np.array(fmri_data[0])\n",
    "print(subject_zero.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.112634075021202, -2.4816354533853904, 6.301162322570788, -0.4318679637650589, -1.5139628065971713]\n",
      "[-0.9263837702103469, -0.9160614702604, 0.5151801601149782, -0.8384103923678308, -0.9219541666858164]\n",
      "[0.5874925274376781, 1.1961929669346083, 0.09873265732566508, -1.0566230101831044, 1.607860846171499]\n",
      "195127\n",
      "177341\n",
      "185703\n"
     ]
    }
   ],
   "source": [
    "print((fmri_data[0][0][0:5]))\n",
    "print((fmri_data[1][0][0:5]))\n",
    "print((fmri_data[2][0][0:5]))\n",
    "print(len(fmri_data[0][0]))\n",
    "print(len(fmri_data[1][0]))\n",
    "print(len(fmri_data[2][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 384, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Choose the number of components to keep\n",
    "n_components = 10  # Example: Keeping the top 10 components\n",
    "\n",
    "subjects_pca_data = []\n",
    "\n",
    "for subject in fmri_data:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    # Flatten the data for PCA if it's 3D (time x width x height) or 4D (time x width x height x depth)\n",
    "    # For simplicity, we assume 2D data (time x features). Adjust the reshape accordingly for 3D/4D data.\n",
    "    # subject_flat = subject.reshape(subject.shape[0], -1)  # Reshape to (time, features)\n",
    "    subject_pca = pca.fit_transform(subject)\n",
    "\n",
    "    subjects_pca_data.append(np.array(subject_pca))\n",
    "\n",
    "subjects_pca_data = np.array(subjects_pca_data)\n",
    "print(subjects_pca_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 10)\n",
      "(384, 10)\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "(384, 10)\n",
      "(384, 10)\n",
      "Fitting 0 folds for each of 8 candidates, totalling 0 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No fits were performed. Was the CV iterator empty? Were there no candidates?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 77\u001b[0m\n\u001b[0;32m     68\u001b[0m gs \u001b[38;5;241m=\u001b[39m GridSearchCV(Ridge(fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m     69\u001b[0m                   {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: ALPHAS}, cv\u001b[38;5;241m=\u001b[39minner_cv, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Replicate Y to match the expanded X_train\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Since we concatenated sentence data for 7 subjects, replicate Y 7 times\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Y_train = np.tile(Y, (len(train_index), 1))  # Replicating Y for each subject in train_index\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Fit on the training set\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[43mgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[0;32m     80\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:863\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No fits were performed. Was the CV iterator empty? Were there no candidates?"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import time\n",
    "\n",
    "\n",
    "# Function to create a custom cross-validator from pre-defined folds\n",
    "def custom_cv(folds):\n",
    "    for i in range(len(folds)):\n",
    "        train_indices = np.concatenate([folds[j] for j in range(len(folds)) if j != i])\n",
    "        test_indices = folds[i]\n",
    "        yield train_indices, test_indices\n",
    "\n",
    "# Use the custom cross-validator for your inner CV\n",
    "inner_cv = custom_cv(fold_indices)\n",
    "\n",
    "# # For inner CV\n",
    "# n_folds = 5  \n",
    "# state = int(time.time())\n",
    "# inner_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "# Candidate ridge regression regularization parameters.\n",
    "ALPHAS = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e1]\n",
    "\n",
    "# Assuming `subjects` is a list or array that indicates the subject number for each sample in X\n",
    "# For example, subjects = [0, 0, ..., 1, 1, ..., 7, 7] if the first set of rows in X belong to subject 0, and so on.\n",
    "# Adjust this to match how your data is structured.\n",
    "\n",
    "# LeaveOneSubjectOut is not directly available, so we simulate it using LeaveOneOut,\n",
    "# assuming each \"fold\" is a subject.\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Placeholder for decoder predictions\n",
    "decoder_predictions = []\n",
    "\n",
    "# Placeholder for test indices\n",
    "test_indices = []\n",
    "\n",
    "Y = embeddings - embeddings.mean(axis=0)\n",
    "Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)\n",
    "\n",
    "# Loop over each fold (subject) for leave-one-subject-out CV\n",
    "for train_index, test_index in loo.split(np.arange(8)):\n",
    "\n",
    "    # # Extract training and test data for this fold\n",
    "    # # This concatenates along the sentence dimension for training data\n",
    "    # X_train = np.concatenate(subjects_pca_data[train_index], axis=0)\n",
    "\n",
    "    # Extract training data for components and take the mean across subjects\n",
    "    # This averages the component data across the 7 training subjects for each sentence\n",
    "    X_train = np.mean(subjects_pca_data[train_index], axis=0)  # Taking mean across the subject dimension\n",
    "\n",
    "\n",
    "    X_test = subjects_pca_data[test_index].squeeze(axis=0)\n",
    "\n",
    "    print(X_test.shape)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    X_train = X_train - X_train.mean(axis=0)\n",
    "    X_train = X_train / np.linalg.norm(X_train, axis=1, keepdims=True)\n",
    "    X_test = X_test - X_test.mean(axis=0)\n",
    "    X_test = X_test / np.linalg.norm(X_test, axis=1, keepdims=True)\n",
    "    \n",
    "    # Run inner CV for hyperparameter tuning\n",
    "    gs = GridSearchCV(Ridge(fit_intercept=False),\n",
    "                      {\"alpha\": ALPHAS}, cv=inner_cv, n_jobs=n_jobs, verbose=10)\n",
    "    \n",
    "\n",
    "    # Replicate Y to match the expanded X_train\n",
    "    # Since we concatenated sentence data for 7 subjects, replicate Y 7 times\n",
    "    # Y_train = np.tile(Y, (len(train_index), 1))  # Replicating Y for each subject in train_index\n",
    "\n",
    "    # Fit on the training set\n",
    "    gs.fit(X_train, Y)\n",
    "\n",
    "    # Predict on the test set\n",
    "    Y_pred = gs.predict(X_test)\n",
    "\n",
    "    # Store predictions\n",
    "    decoder_predictions.append(Y_pred)\n",
    "    \n",
    "\n",
    "# Convert predictions list to an array for further analysis\n",
    "decoder_predictions = np.array(decoder_predictions)\n",
    "decoder_predictions_concat = np.concatenate(decoder_predictions)\n",
    "\n",
    "print(decoder_predictions.shape)\n",
    "print(decoder_predictions_concat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient 0: 0.0067918952232561445\n",
      "Pairwise Accuracy 0: 0.5043380113141862\n",
      "Pearson Correlation Coefficient 1: 0.017882779115256358\n",
      "Pairwise Accuracy 1: 0.522927545691906\n",
      "Pearson Correlation Coefficient 2: 0.019916078379573886\n",
      "Pairwise Accuracy 2: 0.5302164926022629\n",
      "Pearson Correlation Coefficient 3: 0.028737449491400235\n",
      "Pairwise Accuracy 3: 0.5516345735422106\n",
      "Pearson Correlation Coefficient 4: 0.006331650129180452\n",
      "Pairwise Accuracy 4: 0.5232131201044387\n",
      "Pearson Correlation Coefficient 5: -0.036789915591514115\n",
      "Pairwise Accuracy 5: 0.4618146214099217\n",
      "Pearson Correlation Coefficient 6: 0.000585922346742577\n",
      "Pairwise Accuracy 6: 0.5036988685813751\n",
      "Pearson Correlation Coefficient 7: 4.379542212248321e-05\n",
      "Pairwise Accuracy 7: 0.50707136640557\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "### Evaluate for each subject.\n",
    "\n",
    "for subject in np.arange(8):\n",
    "    predictions = decoder_predictions[subject]\n",
    "\n",
    "    # Evaluate the performance (e.g., using mean squared error)\n",
    "    mse = mean_squared_error(Y, predictions)\n",
    "    # print(f\"Mean Squared Error {subject}: {mse}\")\n",
    "\n",
    "    r2 = r2_score(Y, predictions)\n",
    "    # print(f\"R-squared (R2) Score {subject}: {r2}\")\n",
    "\n",
    "    # Pearson Correlation Coefficient\n",
    "    res = [pearsonr(t, p).statistic for t, p in zip(Y, predictions)]\n",
    "    pearson_corr = np.mean(res)\n",
    "    print(f\"Pearson Correlation Coefficient {subject}: {pearson_corr}\")\n",
    "\n",
    "    # Cosine Similarity\n",
    "    cosine_sim = np.mean(cosine_similarity(predictions, Y))\n",
    "    # print(f\"Cosine Similarity {subject}: {cosine_sim}\")\n",
    "\n",
    "    # Pairwise Accuracy\n",
    "    res = [ cosine(predictions[i], Y[i]) + cosine(predictions[j], Y[j]) < cosine(predictions[i], Y[j]) + cosine(predictions[j], Y[i])\n",
    "            for i in range(len(predictions))\n",
    "            for j in range(i + 1, len(predictions))\n",
    "        ]\n",
    "    pairwise_acc = sum(res) / len(res)\n",
    "    print(f\"Pairwise Accuracy {subject}: {pairwise_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings\n",
    "# embeddings_normed = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nested CV.\n",
    "# Inner CV is responsible for hyperparameter optimization;\n",
    "# outer CV is responsible for prediction.\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "state = int(time.time())\n",
    "inner_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "\n",
    "# Final data prep: normalize.\n",
    "X = voxels - voxels.mean(axis=0)\n",
    "X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "Y = embeddings - embeddings.mean(axis=0)\n",
    "Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Run learning.\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "# Candidate ridge regression regularization parameters.\n",
    "ALPHAS = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e1]\n",
    "\n",
    "# Run inner CV.\n",
    "gs = GridSearchCV(Ridge(fit_intercept=False),\n",
    "                {\"alpha\": ALPHAS}, cv=inner_cv, n_jobs=n_jobs, verbose=10)\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Line\n",
    "\n",
    "Nested Cross-Validation:\n",
    "\n",
    "The use of cross_val_predict with GridSearchCV (gs in this context) as the estimator \n",
    "is a part of a nested cross-validation strategy. \n",
    "The key purpose here is to evaluate the model's performance in a way that is as unbiased as possible.\n",
    "\n",
    "Independent Data Splits:\n",
    "\n",
    "The outer cross-validation (cv=outer_cv) splits the dataset into training and test sets multiple times \n",
    "(based on the number of folds in outer_cv). For each of these splits, \n",
    "the inner cross-validation (within GridSearchCV) finds the best alpha value. \n",
    "\n",
    "This process ensures that the choice of hyperparameters (alpha in this case) is not biased by the \n",
    "particular split of data used for model training and evaluation.\n",
    "\n",
    "Generating Unbiased Predictions:\n",
    "\n",
    "cross_val_predict does not simply fit the model but generates predictions for each point \n",
    "when it is in the test set of the outer cross-validation. \n",
    "These predictions are made by a model that has never seen the data point during training, \n",
    "thereby providing an unbiased estimate of the model's performance on unseen data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run outer CV.\n",
    "decoder_predictions = cross_val_predict(gs, X, Y, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(decoder_predictions.shape)\n",
    "# print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pairwise_accuracy(y_true, y_pred):\n",
    "#     n = len(y_true)\n",
    "#     correct_pairs = 0\n",
    "#     total_pairs = 0\n",
    "\n",
    "#     for i in range(n):\n",
    "#         for j in range(i + 1, n):\n",
    "#             if (y_true[i] > y_true[j]) == (y_pred[i] > y_pred[j]):\n",
    "#                 correct_pairs += 1\n",
    "#             total_pairs += 1\n",
    "\n",
    "#     return correct_pairs / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "# # Example usage:\n",
    "# # y_true = [actual values]\n",
    "# # y_pred = [predicted values]\n",
    "# # accuracy = pairwise_accuracy(y_true, y_pred)\n",
    "# # print(\"Pairwise Accuracy:\", accuracy)\n",
    "\n",
    "# def pairwise_accuracy_efficient(y_true, y_pred):\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "\n",
    "#     # Create a matrix of differences for true labels\n",
    "#     diff_true = np.subtract.outer(y_true, y_true) > 0\n",
    "\n",
    "#     # Create a matrix of differences for predictions\n",
    "#     diff_pred = np.subtract.outer(y_pred, y_pred) > 0\n",
    "\n",
    "#     # Count the number of pairs that agree in order\n",
    "#     correct_pairs = np.sum(diff_true == diff_pred)\n",
    "\n",
    "#     # Total number of pairs\n",
    "#     total_pairs = len(y_true) * (len(y_true) - 1) / 2\n",
    "\n",
    "#     print(correct_pairs)\n",
    "#     print(total_pairs)\n",
    "\n",
    "#     return correct_pairs / total_pairs if total_pairs > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, clone\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pairwise_accuracy(\n",
    "#     estimator: BaseEstimator = None,\n",
    "#     X: torch.Tensor = None,  # noqa\n",
    "#     y: torch.Tensor = None,\n",
    "#     topic_ids: torch.Tensor = None,\n",
    "#     scoring_variation: str = None,  # type: ignore\n",
    "# ) -> float:\n",
    "#     \"\"\"Calculate the average pairwise accuracy of all pairs of true and predicted vectors.\n",
    "\n",
    "#     Based on the pairwise accuracy as defined in Oota et al. 2022, Sun et al. 2021, Pereira et al. 2018.\n",
    "\n",
    "#     :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "#     :type estimator: BaseEstimator\n",
    "#     :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "#     :type X: torch.Tensor\n",
    "#     :param y: True MRI vectors\n",
    "#     :type y: torch.Tensor\n",
    "#     :param topic_ids: Topic IDs for each paragraph\n",
    "#     :type topic_ids: torch.Tensor\n",
    "#     :param scoring_variation: Variation of the scoring function, defaults to None\n",
    "#     :type scoring_variation: str\n",
    "#     :return: Average pairwise accuracy from all possible sentence pairs\n",
    "#     :rtype: float\n",
    "#     \"\"\"\n",
    "#     pred = estimator.predict(X)  # noqa\n",
    "\n",
    "#     if scoring_variation == \"same-topic\":\n",
    "#         # Calculate pairwise accuracy for same-topic sentences\n",
    "#         res = [\n",
    "#             cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "#             for i in range(len(X))\n",
    "#             for j in range(i + 1, len(X)) if topic_ids[i] == topic_ids[j]\n",
    "#         ]\n",
    "#     elif scoring_variation == \"different-topic\":\n",
    "#         # Calculate pairwise accuracy for different-topic sentences\n",
    "#         res = [\n",
    "#             cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "#             for i in range(len(X))\n",
    "#             for j in range(i + 1, len(X)) if topic_ids[i] != topic_ids[j]\n",
    "#         ]\n",
    "#     else:\n",
    "#         # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "#         # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "#         res = [\n",
    "#             cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "#             for i in range(len(X))\n",
    "#             for j in range(i + 1, len(X))\n",
    "#         ]\n",
    "\n",
    "#     # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "#     return sum(res) / len(res)\n",
    "\n",
    "\n",
    "# def pearson_scoring(\n",
    "#     estimator: BaseEstimator = None,\n",
    "#     X: torch.Tensor = None,  # noqa\n",
    "#     y: torch.Tensor = None,\n",
    "# ) -> float:\n",
    "#     \"\"\"Calculate the average pearson correlation for the given set of true and predicted MRI vectors.\n",
    "\n",
    "#     :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "#     :type estimator: BaseEstimator\n",
    "#     :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "#     :type X: torch.Tensor\n",
    "#     :param y: True MRI vectors\n",
    "#     :type y: torch.Tensor\n",
    "#     :return: Average pearson correlation from all pairs of predicted and true MRI vectors\n",
    "#     :rtype: float\n",
    "#     \"\"\"\n",
    "#     pred = estimator.predict(X)  # noqa\n",
    "\n",
    "#     # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "#     # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "#     res = [pearsonr(t, p).statistic for t, p in zip(y, pred)]\n",
    "\n",
    "#     # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "#     return np.mean(res)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Evaluate.\n",
    "\n",
    "Y_flatten = Y.flatten()\n",
    "pred_flatten = decoder_predictions.flatten()\n",
    "\n",
    "# Evaluate the performance (e.g., using mean squared error)\n",
    "mse = mean_squared_error(Y, decoder_predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "r2 = r2_score(Y, decoder_predictions)\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "\n",
    "# Pearson Correlation Coefficient\n",
    "res = [pearsonr(t, p).statistic for t, p in zip(Y, decoder_predictions)]\n",
    "pearson_corr = np.mean(res)\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim = np.mean(cosine_similarity(decoder_predictions, Y))\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "\n",
    "# Pairwise Accuracy\n",
    "res = [ cosine(decoder_predictions[i], Y[i]) + cosine(decoder_predictions[j], Y[j]) < cosine(decoder_predictions[i], Y[j]) + cosine(decoder_predictions[j], Y[i])\n",
    "        for i in range(len(X))\n",
    "        for j in range(i + 1, len(X))\n",
    "    ]\n",
    "pairwise_acc = sum(res) / len(res)\n",
    "print(f\"Pairwise Accuracy: {pairwise_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
