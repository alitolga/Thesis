{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alito\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DebertaV2ForMaskedLM\n",
    "\n",
    "from peft import PeftConfig, PeftModel, AutoPeftModelForCausalLM, LoraConfig, TaskType, PeftModelForCausalLM\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict, GridSearchCV\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all load the PEFT models and get the sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\alito\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# BASELINE MODELS\n",
    "\n",
    "# repo_name = \"bert-base-uncased\"\n",
    "# repo_name = \"roberta-base\"\n",
    "# repo_name = \"microsoft/deberta-base\"\n",
    "# repo_name = \"microsoft/deberta-v3-base\"\n",
    "# repo_name = \"google/electra-base-generator\"\n",
    "# repo_name = \"facebook/bart-base\"\n",
    "# repo_name = \"gpt2\"\n",
    "\n",
    "# MODELS OPTIMIZED WITH PEFT LIBRARY, USING LORA WITH CAUSAL LM AND RANK=8\n",
    "\n",
    "# repo_name = \"alitolga/bert-base-uncased-large-peft\"\n",
    "# repo_name = \"alitolga/roberta-base-large-peft\"\n",
    "# repo_name = \"alitolga/deberta-base-large-peft\"\n",
    "# repo_name = \"alitolga/deberta-v3-base-large-peft\"\n",
    "# repo_name = \"alitolga/electra-base-generator-large-peft\"\n",
    "# repo_name = \"alitolga/bart-base-large-peft\"\n",
    "# repo_name = \"alitolga/gpt2-large-peft\"\n",
    "\n",
    "# config = PeftConfig.from_pretrained(repo_name)\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "# base_model = DebertaV2ForMaskedLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, repo_name, config=config)\n",
    "# model = PeftModel.from_pretrained(base_model, repo_name)\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(repo_name)\n",
    "model = DebertaV2ForMaskedLM.from_pretrained(repo_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
    "\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Try inference just for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "generated_text = text_generator(\"Welcome to the Hugging Face course! I am pleased to inform you\", \n",
    "                                max_length = 50, \n",
    "                                # num_return_sequences=2\n",
    "                                # max_new_tokens = 30\n",
    "                                )\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"helena-balabin/pereira_fMRI_sentences\"\n",
    "\n",
    "# Specify the path to save or load the dataset\n",
    "save_path = \"./data\"\n",
    "\n",
    "# Load the dataset, use the cache if available\n",
    "dataset = load_dataset(dataset_name, cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "sentences = dataset[\"train\"][\"sentences\"]\n",
    "sentences = sentences[0] # 0th subject\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An accordion is a portable musical instrument with two keyboards.\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the sentence embeddings from the Peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    embeddings = torch.mean(hidden_states[0], dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_embeddings(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the Brain Decoding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 195127)\n"
     ]
    }
   ],
   "source": [
    "# Get the voxels. For simplicity we start with all the brain regions\n",
    "fmri_data = dataset[\"train\"][\"all\"]\n",
    "\n",
    "# fMRI data of the first subject out of 8\n",
    "voxels = np.array(fmri_data[0])\n",
    "print(voxels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings\n",
    "# embeddings_normed = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nested CV.\n",
    "# Inner CV is responsible for hyperparameter optimization;\n",
    "# outer CV is responsible for prediction.\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "state = int(time.time())\n",
    "inner_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "\n",
    "# Final data prep: normalize.\n",
    "X = voxels - voxels.mean(axis=0)\n",
    "X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "Y = embeddings - embeddings.mean(axis=0)\n",
    "Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPurpose of This Line\\n\\nNested Cross-Validation:\\n\\nThe use of cross_val_predict with GridSearchCV (gs in this context) as the estimator \\nis a part of a nested cross-validation strategy. \\nThe key purpose here is to evaluate the model's performance in a way that is as unbiased as possible.\\n\\nIndependent Data Splits:\\n\\nThe outer cross-validation (cv=outer_cv) splits the dataset into training and test sets multiple times \\n(based on the number of folds in outer_cv). For each of these splits, \\nthe inner cross-validation (within GridSearchCV) finds the best alpha value. \\n\\nThis process ensures that the choice of hyperparameters (alpha in this case) is not biased by the \\nparticular split of data used for model training and evaluation.\\n\\nGenerating Unbiased Predictions:\\n\\ncross_val_predict does not simply fit the model but generates predictions for each point \\nwhen it is in the test set of the outer cross-validation. \\nThese predictions are made by a model that has never seen the data point during training, \\nthereby providing an unbiased estimate of the model's performance on unseen data.\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Run learning.\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "# Candidate ridge regression regularization parameters.\n",
    "ALPHAS = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e1]\n",
    "\n",
    "# Run inner CV.\n",
    "gs = GridSearchCV(Ridge(fit_intercept=False),\n",
    "                {\"alpha\": ALPHAS}, cv=inner_cv, n_jobs=n_jobs, verbose=10)\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Line\n",
    "\n",
    "Nested Cross-Validation:\n",
    "\n",
    "The use of cross_val_predict with GridSearchCV (gs in this context) as the estimator \n",
    "is a part of a nested cross-validation strategy. \n",
    "The key purpose here is to evaluate the model's performance in a way that is as unbiased as possible.\n",
    "\n",
    "Independent Data Splits:\n",
    "\n",
    "The outer cross-validation (cv=outer_cv) splits the dataset into training and test sets multiple times \n",
    "(based on the number of folds in outer_cv). For each of these splits, \n",
    "the inner cross-validation (within GridSearchCV) finds the best alpha value. \n",
    "\n",
    "This process ensures that the choice of hyperparameters (alpha in this case) is not biased by the \n",
    "particular split of data used for model training and evaluation.\n",
    "\n",
    "Generating Unbiased Predictions:\n",
    "\n",
    "cross_val_predict does not simply fit the model but generates predictions for each point \n",
    "when it is in the test set of the outer cross-validation. \n",
    "These predictions are made by a model that has never seen the data point during training, \n",
    "thereby providing an unbiased estimate of the model's performance on unseen data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "# Run outer CV.\n",
    "decoder_predictions = cross_val_predict(gs, X, Y, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384, 768)\n",
      "(384, 768)\n"
     ]
    }
   ],
   "source": [
    "print(decoder_predictions.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pairwise_accuracy(y_true, y_pred):\n",
    "#     n = len(y_true)\n",
    "#     correct_pairs = 0\n",
    "#     total_pairs = 0\n",
    "\n",
    "#     for i in range(n):\n",
    "#         for j in range(i + 1, n):\n",
    "#             if (y_true[i] > y_true[j]) == (y_pred[i] > y_pred[j]):\n",
    "#                 correct_pairs += 1\n",
    "#             total_pairs += 1\n",
    "\n",
    "#     return correct_pairs / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "# # Example usage:\n",
    "# # y_true = [actual values]\n",
    "# # y_pred = [predicted values]\n",
    "# # accuracy = pairwise_accuracy(y_true, y_pred)\n",
    "# # print(\"Pairwise Accuracy:\", accuracy)\n",
    "\n",
    "# def pairwise_accuracy_efficient(y_true, y_pred):\n",
    "#     y_true = np.array(y_true)\n",
    "#     y_pred = np.array(y_pred)\n",
    "\n",
    "#     # Create a matrix of differences for true labels\n",
    "#     diff_true = np.subtract.outer(y_true, y_true) > 0\n",
    "\n",
    "#     # Create a matrix of differences for predictions\n",
    "#     diff_pred = np.subtract.outer(y_pred, y_pred) > 0\n",
    "\n",
    "#     # Count the number of pairs that agree in order\n",
    "#     correct_pairs = np.sum(diff_true == diff_pred)\n",
    "\n",
    "#     # Total number of pairs\n",
    "#     total_pairs = len(y_true) * (len(y_true) - 1) / 2\n",
    "\n",
    "#     print(correct_pairs)\n",
    "#     print(total_pairs)\n",
    "\n",
    "#     return correct_pairs / total_pairs if total_pairs > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, clone\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def pairwise_accuracy(\n",
    "    estimator: BaseEstimator = None,\n",
    "    X: torch.Tensor = None,  # noqa\n",
    "    y: torch.Tensor = None,\n",
    "    topic_ids: torch.Tensor = None,\n",
    "    scoring_variation: str = None,  # type: ignore\n",
    ") -> float:\n",
    "    \"\"\"Calculate the average pairwise accuracy of all pairs of true and predicted vectors.\n",
    "\n",
    "    Based on the pairwise accuracy as defined in Oota et al. 2022, Sun et al. 2021, Pereira et al. 2018.\n",
    "\n",
    "    :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "    :type estimator: BaseEstimator\n",
    "    :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "    :type X: torch.Tensor\n",
    "    :param y: True MRI vectors\n",
    "    :type y: torch.Tensor\n",
    "    :param topic_ids: Topic IDs for each paragraph\n",
    "    :type topic_ids: torch.Tensor\n",
    "    :param scoring_variation: Variation of the scoring function, defaults to None\n",
    "    :type scoring_variation: str\n",
    "    :return: Average pairwise accuracy from all possible sentence pairs\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    pred = estimator.predict(X)  # noqa\n",
    "\n",
    "    if scoring_variation == \"same-topic\":\n",
    "        # Calculate pairwise accuracy for same-topic sentences\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X)) if topic_ids[i] == topic_ids[j]\n",
    "        ]\n",
    "    elif scoring_variation == \"different-topic\":\n",
    "        # Calculate pairwise accuracy for different-topic sentences\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X)) if topic_ids[i] != topic_ids[j]\n",
    "        ]\n",
    "    else:\n",
    "        # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "        # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X))\n",
    "        ]\n",
    "\n",
    "    # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "    return sum(res) / len(res)\n",
    "\n",
    "\n",
    "def pearson_scoring(\n",
    "    estimator: BaseEstimator = None,\n",
    "    X: torch.Tensor = None,  # noqa\n",
    "    y: torch.Tensor = None,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the average pearson correlation for the given set of true and predicted MRI vectors.\n",
    "\n",
    "    :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "    :type estimator: BaseEstimator\n",
    "    :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "    :type X: torch.Tensor\n",
    "    :param y: True MRI vectors\n",
    "    :type y: torch.Tensor\n",
    "    :return: Average pearson correlation from all pairs of predicted and true MRI vectors\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    pred = estimator.predict(X)  # noqa\n",
    "\n",
    "    # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "    # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "    res = [pearsonr(t, p).statistic for t, p in zip(y, pred)]\n",
    "\n",
    "    # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "    return np.mean(res)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0012917273580361974\n",
      "R-squared (R2) Score: 0.007150983612804379\n",
      "Pearson Correlation Coefficient: 0.09380054628315744\n",
      "Cosine Similarity: -3.174795188503638e-05\n",
      "Pairwise Accuracy: 0.7945088120104439\n"
     ]
    }
   ],
   "source": [
    "######### Evaluate.\n",
    "\n",
    "Y_flatten = Y.flatten()\n",
    "pred_flatten = decoder_predictions.flatten()\n",
    "\n",
    "# Evaluate the performance (e.g., using mean squared error)\n",
    "mse = mean_squared_error(Y, decoder_predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "r2 = r2_score(Y, decoder_predictions)\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "\n",
    "# Pearson Correlation Coefficient\n",
    "res = [pearsonr(t, p).statistic for t, p in zip(Y, decoder_predictions)]\n",
    "pearson_corr = np.mean(res)\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim = np.mean(cosine_similarity(decoder_predictions, Y))\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "\n",
    "# Pairwise Accuracy\n",
    "res = [ cosine(decoder_predictions[i], Y[i]) + cosine(decoder_predictions[j], Y[j]) < cosine(decoder_predictions[i], Y[j]) + cosine(decoder_predictions[j], Y[i])\n",
    "        for i in range(len(X))\n",
    "        for j in range(i + 1, len(X))\n",
    "    ]\n",
    "pairwise_acc = sum(res) / len(res)\n",
    "print(f\"Pairwise Accuracy: {pairwise_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
