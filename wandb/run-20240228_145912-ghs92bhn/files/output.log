
You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
{'loss': 10.4695, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.2}
{'loss': 10.035, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}
{'loss': 9.4092, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.6}
{'loss': 8.9099, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}
{'loss': 8.6306, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 8.496716499328613, 'eval_runtime': 2.825, 'eval_samples_per_second': 353.988, 'eval_steps_per_second': 44.249, 'epoch': 1.0}
{'loss': 8.5137, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 8.4604, 'learning_rate': 1.0666666666666667e-05, 'epoch': 1.4}
{'loss': 8.4186, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}
{'loss': 8.4027, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.8}
{'loss': 8.3839, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}
{'eval_loss': 8.328328132629395, 'eval_runtime': 2.7339, 'eval_samples_per_second': 365.78, 'eval_steps_per_second': 45.723, 'epoch': 2.0}
{'loss': 8.3793, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.2}
{'loss': 8.3856, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}
{'loss': 8.3853, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.6}
{'loss': 8.3768, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}
{'loss': 8.3841, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 8.315900802612305, 'eval_runtime': 2.8572, 'eval_samples_per_second': 349.987, 'eval_steps_per_second': 43.748, 'epoch': 3.0}
{'train_runtime': 108.5735, 'train_samples_per_second': 110.524, 'train_steps_per_second': 13.816, 'train_loss': 8.769637736002604, 'epoch': 3.0}
You are using a model of type electra to instantiate a model of type deberta-v2. This is not supported for all configurations of models and can yield errors.
trainable params: 1,572,864 || all params: 35,312,186 || trainable%: 4.454167748210207
cpu
Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at google/electra-base-generator and are newly initialized: ['encoder.layer.3.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.key_proj.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key_proj.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value_proj.weight', 'encoder.layer.10.attention.self.query_proj.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.self.value_proj.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'cls.predictions.decoder.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query_proj.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.7.attention.self.query_proj.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.value_proj.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value_proj.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.2.attention.self.value_proj.bias', 'encoder.layer.6.attention.self.key_proj.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query_proj.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query_proj.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key_proj.bias', 'encoder.layer.10.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query_proj.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.value_proj.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.2.attention.self.query_proj.bias', 'encoder.layer.3.attention.self.value_proj.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.1.attention.self.key_proj.weight', 'encoder.layer.7.attention.self.key_proj.bias', 'encoder.layer.10.attention.self.key_proj.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value_proj.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.9.attention.self.value_proj.weight', 'encoder.layer.5.attention.output.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.attention.self.value_proj.bias', 'encoder.layer.8.attention.self.value_proj.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.attention.self.query_proj.weight', 'encoder.layer.4.attention.self.query_proj.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.key_proj.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key_proj.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value_proj.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.11.attention.self.key_proj.weight', 'encoder.layer.5.attention.self.key_proj.bias', 'encoder.layer.3.attention.self.key_proj.bias', 'encoder.layer.9.attention.self.query_proj.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.7.attention.self.query_proj.bias', 'encoder.layer.9.attention.self.key_proj.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.2.attention.self.query_proj.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.11.attention.self.key_proj.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query_proj.weight', 'encoder.layer.3.attention.self.query_proj.bias', 'cls.predictions.transform.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'cls.predictions.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key_proj.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.attention.self.value_proj.weight', 'encoder.layer.5.attention.self.query_proj.bias', 'encoder.layer.8.attention.self.query_proj.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.self.value_proj.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.10.attention.self.key_proj.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.7.attention.self.key_proj.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.self.key_proj.bias', 'encoder.layer.6.attention.self.query_proj.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value_proj.bias', 'encoder.layer.5.attention.self.value_proj.weight', 'encoder.layer.10.attention.self.query_proj.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query_proj.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value_proj.weight', 'encoder.layer.10.attention.self.value_proj.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.attention.self.value_proj.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.self.value_proj.weight', 'encoder.layer.4.attention.self.value_proj.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key_proj.bias', 'encoder.layer.2.attention.self.key_proj.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.self.query_proj.weight', 'encoder.layer.0.attention.self.query_proj.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.6.attention.self.key_proj.bias', 'encoder.layer.0.attention.self.value_proj.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key_proj.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.attention.self.query_proj.bias', 'cls.predictions.decoder.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'embeddings.embed_proj.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key_proj.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query_proj.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
cpu
Using device: cuda
cuda:0
{'loss': 10.1557, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.2}
{'loss': 9.7731, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}
{'loss': 9.244, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.6}
{'loss': 8.8771, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}
{'loss': 8.6941, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}
{'eval_loss': 8.575545310974121, 'eval_runtime': 2.7267, 'eval_samples_per_second': 366.745, 'eval_steps_per_second': 45.843, 'epoch': 1.0}
{'loss': 8.6035, 'learning_rate': 1.2e-05, 'epoch': 1.2}
{'loss': 8.5423, 'learning_rate': 1.0666666666666667e-05, 'epoch': 1.4}
{'loss': 8.5017, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}
{'loss': 8.4845, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.8}
{'loss': 8.4518, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}
{'eval_loss': 8.388410568237305, 'eval_runtime': 3.0041, 'eval_samples_per_second': 332.881, 'eval_steps_per_second': 41.61, 'epoch': 2.0}
{'loss': 8.4531, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.2}
{'loss': 8.4484, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}
{'loss': 8.4268, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.6}
{'loss': 8.4503, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}
{'loss': 8.424, 'learning_rate': 0.0, 'epoch': 3.0}
{'eval_loss': 8.362646102905273, 'eval_runtime': 2.9569, 'eval_samples_per_second': 338.194, 'eval_steps_per_second': 42.274, 'epoch': 3.0}
