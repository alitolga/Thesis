{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is a script I added, in case I need to use a manual training loop. This is not the script I am currently using for my thesis, I am training the models using the code in training_models_causalLM_new.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model \n",
    "from transformers import BertTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding, AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config_bert = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bert_model_unused = AutoModelForCausalLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_model_peft = get_peft_model(bert_model_unused, peft_config_bert)\n",
    "bert_model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"bookcorpus\"\n",
    "\n",
    "# Specify the path to save or load the dataset\n",
    "save_path = \"./data\"\n",
    "\n",
    "# Load the dataset, use the cache if available\n",
    "dataset = load_dataset(dataset_name, cache_dir=save_path)\n",
    "\n",
    "bookcorpus_dataset = dataset[\"train\"][\"text\"][:40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bookcorpus_dataset))\n",
    "print(bookcorpus_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_inputs = [bert_tokenizer(sentence, return_tensors='pt', truncation=True, padding=\"max_length\")\n",
    "                    # for sentence in bookcorpus_dataset]\n",
    "tokenized_inputs = bert_tokenizer(bookcorpus_dataset, return_tensors='pt', truncation=True, padding=True)\n",
    "print(type(tokenized_inputs))\n",
    "print(tokenized_inputs['input_ids'].shape)\n",
    "\n",
    "# visited = []\n",
    "# for dictionary in tokenized_inputs['attention_mask']:\n",
    "#     if len(dictionary) not in visited:\n",
    "#         print(dictionary.shape)\n",
    "#         visited.append(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"mymodels/bert-lora-casual-lm\",\n",
    "#     learning_rate=1e-3,\n",
    "#     # per_device_train_batch_size=32,\n",
    "#     # per_device_eval_batch_size=32,\n",
    "#     # num_train_epochs=2,\n",
    "#     # weight_decay=0.01,\n",
    "#     # evaluation_strategy=\"epoch\",\n",
    "#     # save_strategy=\"epoch\",\n",
    "#     # load_best_model_at_end=True,\n",
    "#     push_to_hub=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model=bert_model_peft,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_inputs,\n",
    "#     # tokenizer=bert_tokenizer,\n",
    "#     # data_collator=DataCollatorWithPadding(tokenizer=bert_tokenizer)\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, batch_encoding):\n",
    "        self.batch_encoding = batch_encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_encoding.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extract tensors from BatchEncoding\n",
    "        input_ids = self.batch_encoding.input_ids[index]\n",
    "        attention_mask = self.batch_encoding.attention_mask[index]\n",
    "        token_type_ids = self.batch_encoding.token_type_ids[index]\n",
    "\n",
    "        # Convert to dictionary\n",
    "        inputs_dict = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids\n",
    "        }\n",
    "\n",
    "        return inputs_dict\n",
    "    \n",
    "# Assuming batch_encoding is your BatchEncoding object\n",
    "training_dataset = CustomDataset(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_peft.to('cuda')\n",
    "\n",
    "num_train_epochs = 500\n",
    "per_device_train_batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "dataloader = DataLoader(training_dataset, batch_size=per_device_train_batch_size, shuffle=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(bert_model_peft.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_train_epochs):\n",
    "    bert_model_peft.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_train_epochs}\"):\n",
    "        inputs = {key: value.to('cuda') for key, value in batch.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = bert_model_peft(**inputs, labels=inputs[\"input_ids\"])\n",
    "        # outputs = bert_model_peft(**inputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_train_epochs}, Average Loss: {average_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_peft.save_pretrained(\"models/bert-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
