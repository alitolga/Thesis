{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Double checking we are using the GPU on the VSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "# Check if CUDA is available and set the device to GPU if it is\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the bookcorpus dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "save_path = \"./data\"\n",
    "\n",
    "bookcorpus_dataset = load_dataset(\"bookcorpus\", split=\"train[:5000]\", cache_dir=save_path)\n",
    "bookcorpus_dataset = bookcorpus_dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcorpus_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcorpus_dataset[\"train\"]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.lang.en import English\n",
    "# import random\n",
    "\n",
    "# # Load the spaCy model for English\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def generate_question_answer_pairs(sentences):\n",
    "#     \"\"\"\n",
    "#     Generate question-answer pairs from a list of sentences using Named Entity Recognition for identifying answers\n",
    "#     and a simple template for question generation.\n",
    "#     \"\"\"\n",
    "#     qa_pairs = []\n",
    "#     for sentence in sentences:\n",
    "#         doc = nlp(sentence)\n",
    "#         entities = [ent for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'DATE', 'GPE', 'EVENT']]\n",
    "#         for ent in entities:\n",
    "#             # # Customize the question based on the entity type for better relevance\n",
    "#             # if ent.label_ == 'PERSON':\n",
    "#             #     question = f\"Who is {ent.text}?\"\n",
    "#             # elif ent.label_ == 'DATE':\n",
    "#             #     question = f\"When did it happen?\"\n",
    "#             # elif ent.label_ in ['ORG', 'GPE', 'EVENT']:\n",
    "#             #     question = f\"What is mentioned about {ent.text}?\"\n",
    "#             # else:\n",
    "#             question = f\"What is mentioned about {ent.text}?\"\n",
    "#             answer = sentence.replace(ent.text, '')\n",
    "#             qa_pairs.append({\"context\": sentence, \"question\": question, \"answer\": answer})\n",
    "#     return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage:\n",
    "# sentences = bookcorpus_dataset[\"train\"]['text']\n",
    "\n",
    "# # Generate question-answer pairs\n",
    "# qa_pairs = generate_question_answer_pairs(sentences)\n",
    "\n",
    "# for pair in qa_pairs:\n",
    "#     print(\"Context:\", pair[\"context\"])\n",
    "#     print(\"Question:\", pair[\"question\"])\n",
    "#     print(\"Answer:\", pair[\"answer\"])\n",
    "#     print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select the model to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = \"microsoft/deberta-v3-base\"\n",
    "# modelname = \"google/electra-base-generator\"\n",
    "\n",
    "# modelname = \"bert-base-uncased\"\n",
    "# modelname = \"gpt2\"\n",
    "# modelname = \"roberta-base\"\n",
    "# modelname = \"microsoft/deberta-base\"\n",
    "# modelname = \"facebook/bart-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Token Classification Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_and_align_labels(examples):\n",
    "#     tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "#     labels = []\n",
    "#     for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "#         word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "#         previous_word_idx = None\n",
    "#         label_ids = []\n",
    "#         for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "#             if word_idx is None:\n",
    "#                 label_ids.append(-100)\n",
    "#             elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "#                 label_ids.append(label[word_idx])\n",
    "#             else:\n",
    "#                 label_ids.append(-100)\n",
    "#             previous_word_idx = word_idx\n",
    "#         labels.append(label_ids)\n",
    "\n",
    "#     tokenized_inputs[\"labels\"] = labels\n",
    "#     return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Function 1 - Map the data to the tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(tokenizer, examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "partial_tokenize_function = partial(preprocess_function, tokenizer)\n",
    "\n",
    "tokenized_bookcorpus = bookcorpus_dataset.map(\n",
    "    partial_tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=bookcorpus_dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookcorpus_dataset[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_bookcorpus['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Function 2 - Divide the dataset into blocks of block size. Drop the remainder if the length of the dataset is not fully divisible to the block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    block_size = 128\n",
    "\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_bookcorpus.map(group_texts, batched=True, num_proc=4)\n",
    "lm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the LoRA library from PEFT. Set it's parameters and load the model optimized using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False, \n",
    "    r=8,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the reduced number of parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DebertaV2ForMaskedLM, DebertaV2ForQuestionAnswering\n",
    "from transformers import DebertaV2ForSequenceClassification, AutoModelForQuestionAnswering, AutoModelForSeq2SeqLM\n",
    "from transformers import EncoderDecoderModel, AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "\n",
    "# from simpletransformers.seq2seq import Seq2SeqModel\n",
    "\n",
    "# model_without_peft = EncoderDecoderModel.from_encoder_decoder_pretrained(modelname, modelname)\n",
    "\n",
    "# model_without_peft = Seq2SeqModel(\n",
    "#     encoder_type=\"auto\",\n",
    "#     encoder_decoder_name=modelname,\n",
    "#     use_cuda=True,\n",
    "# )\n",
    "\n",
    "# model_without_peft = AutoModelForCausalLM.from_pretrained(modelname)\n",
    "# model_without_peft = AutoModelForQuestionAnswering.from_pretrained(modelname)\n",
    "# model_without_peft = AutoModelForSeq2SeqLM.from_pretrained(modelname)\n",
    "# model_without_peft = AutoModelForSequenceClassification.from_pretrained(modelname)\n",
    "model_without_peft = AutoModelForTokenClassification.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "# model_without_peft = DebertaV2ForMaskedLM.from_pretrained(modelname)\n",
    "# model_without_peft = DebertaV2ForQuestionAnswering.from_pretrained(modelname)\n",
    "# model_without_peft = DebertaV2ForSequenceClassification.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "\n",
    "model = get_peft_model(model_without_peft, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import a Data Collator Function for (Causal) LM. This function will ensure that for each token, we have the following token respective to it as it's label/target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq, DataCollatorForTokenClassification\n",
    "from transformers import DataCollatorForPermutationLanguageModeling, DataCollatorWithPadding, default_data_collator\n",
    "from transformers import DataCollatorForSOP, DataCollatorForWholeWordMask\n",
    "\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "# data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary modifications for Question Answering Task\n",
    "\n",
    "These use the 'question', 'context' and 'answer' columns that are generated during preprocessing before. We tokenize those columns, and pad them accordingly to have the same length. We also use the DataLoader class to give it as an input for the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ask_question(question, context):\n",
    "#     \"\"\"\n",
    "#     Use the pre-trained ELECTRA model to infer an answer to a question given some context.\n",
    "#     This function doesn't require explicit answer annotations.\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "#     input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "#     text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "#     outputs = model(**inputs)\n",
    "#     answer_start_scores = outputs.start_logits\n",
    "#     answer_end_scores = outputs.end_logits\n",
    "\n",
    "#     # Get the most likely beginning and end of answer with the argmax of the score\n",
    "#     answer_start = torch.argmax(answer_start_scores)\n",
    "#     answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "#     # Convert the tokens to the answer string\n",
    "#     answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    \n",
    "#     return answer\n",
    "\n",
    "# # Example usage\n",
    "# context = \"The history of natural language processing (NLP) generally started in the 1950s, although work can be found from earlier periods.\"\n",
    "# question = \"When did NLP start?\"\n",
    "\n",
    "# answer = ask_question(question, context)\n",
    "# print(\"Question:\", question)\n",
    "# print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Question Answering Preprocessing\n",
    "\n",
    "# def tokenize_and_preserve_labels(qa_pair, tokenizer):\n",
    "#     # Tokenize question and context together\n",
    "#     inputs = tokenizer.encode_plus(\n",
    "#         qa_pair[\"question\"],\n",
    "#         qa_pair[\"context\"],\n",
    "#         add_special_tokens=True,\n",
    "#         max_length=512,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         return_offsets_mapping=True,  # Important for mapping token positions to original text\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "\n",
    "#     # Find start and end of answer in tokens\n",
    "#     text = qa_pair[\"context\"]\n",
    "#     answer = qa_pair[\"answer\"]\n",
    "#     start_char = text.find(answer)\n",
    "#     end_char = start_char + len(answer) - 1\n",
    "\n",
    "#     # Map character positions to token positions\n",
    "#     offsets = inputs[\"offset_mapping\"][0].tolist()  # Get the offsets\n",
    "#     answer_token_start, answer_token_end = 0, 0\n",
    "\n",
    "#     # Find tokens that start and end the answer\n",
    "#     for i, offset in enumerate(offsets):\n",
    "#         if start_char >= offset[0] and start_char <= offset[1]:\n",
    "#             answer_token_start = i\n",
    "#         if end_char >= offset[0] and end_char <= offset[1]:\n",
    "#             answer_token_end = i\n",
    "#             break\n",
    "\n",
    "#     # Remove offset mapping to avoid issues during model training\n",
    "#     inputs.pop(\"offset_mapping\")\n",
    "\n",
    "#     return inputs, answer_token_start, answer_token_end\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_qa_pairs, test_qa_pairs = train_test_split(qa_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_tokenized = [tokenize_and_preserve_labels(pair, tokenizer) for pair in train_qa_pairs]\n",
    "# test_tokenized = [tokenize_and_preserve_labels(pair, tokenizer) for pair in test_qa_pairs]\n",
    "\n",
    "# # Now `tokenized_data` contains tokenized inputs along with the start and end positions of the answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class QADataset(Dataset):\n",
    "#     def __init__(self, tokenized_data):\n",
    "#         self.tokenized_data = tokenized_data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.tokenized_data)\n",
    "\n",
    "#     # def __getitem__(self, idx):\n",
    "#     #     return self.tokenized_data[idx]\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_ids = self.tokenized_data[idx][0][\"input_ids\"].squeeze()  # Remove batch dimension\n",
    "#         attention_mask = self.tokenized_data[idx][0][\"attention_mask\"].squeeze()\n",
    "#         start_positions = torch.tensor(self.tokenized_data[idx][1])\n",
    "#         end_positions = torch.tensor(self.tokenized_data[idx][2])\n",
    "        \n",
    "#         return {\n",
    "#             \"input_ids\": input_ids,\n",
    "#             \"attention_mask\": attention_mask,\n",
    "#             \"start_positions\": start_positions,\n",
    "#             \"end_positions\": end_positions\n",
    "#         }\n",
    "\n",
    "# train_dataset = QADataset(train_tokenized)\n",
    "# test_dataset = QADataset(test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tokenizer doesn't have a padding token by default, use End of Sequence Token. If it also doesn't have that, then we have to use a Separator or a Classification token..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.cls_token\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that we are running the model on Gpu and not on Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMDB Database Stuff for Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# def preprocess_function2(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# tokenized_imdb = imdb.map(preprocess_function2, batched=True)\n",
    "\n",
    "# tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rename_label_to_labels(example):\n",
    "#     # This function will be applied to each example. It simply copies the value from 'label' to 'labels'.\n",
    "#     example['labels'] = example['label']\n",
    "#     return example\n",
    "\n",
    "# # Apply the function across all splits in the dataset\n",
    "# tokenized_imdb = tokenized_imdb.map(rename_label_to_labels, remove_columns=['label'])\n",
    "# tokenized_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"mymodels/{modelname}-TokenClassificationWnut\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"all\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally create the Trainer class and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut['train'],\n",
    "    eval_dataset=tokenized_wnut['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Perplexity: {(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally push the model to the Huggingface Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(f\"{modelname}-peft\")\n",
    "# model.save_pretrained(f\"{modelname}-peft-model\")\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
