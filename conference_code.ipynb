{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set the device to GPU if it is\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, PeftModel\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict, GridSearchCV\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the dataset name\n",
    "dataset_name = \"helena-balabin/pereira_fMRI_sentences\"\n",
    "\n",
    "# Specify the path to save or load the dataset\n",
    "save_path = \"./data\"\n",
    "\n",
    "# Load the dataset, use the cache if available\n",
    "pereira_dataset = load_dataset(dataset_name, cache_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pereira_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = \"bert-base-uncased\"\n",
    "modelname = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preprocessing Function 1 - Map the data to the tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(tokenizer, examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"sentences\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "partial_tokenize_function = partial(preprocess_function, tokenizer)\n",
    "\n",
    "tokenized_pereira = pereira_dataset.map(\n",
    "    partial_tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=pereira_dataset['train'].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pereira"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizer Function 2 - Divide the dataset into blocks of block size. Drop the remainder if the length of the dataset is not fully divisible to the block size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    block_size = 128\n",
    "\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = tokenized_pereira.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Collator Function for (Causal) LM. This function will ensure that for each token, we have the following token respective to it as it's label/target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the LoRA library from PEFT. Set it's parameters and load the model optimized using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8,\n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the reduced number of parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_without_peft = AutoModelForCausalLM.from_pretrained(modelname)\n",
    "# model_without_peft = DebertaV2ForMaskedLM.from_pretrained(modelname)\n",
    "\n",
    "model = get_peft_model(model_without_peft, peft_config)\n",
    "\n",
    "model.print_trainable_parameters()\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tokenizer doesn't have a padding token by default, use End of Sequence Token. If it also doesn't have that, then we have to use a Separator or a Classification token..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token = tokenizer.cls_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that we are running the model on Gpu and not on Cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(dataset, test_size=0.2, seed=None):\n",
    "    \"\"\"\n",
    "    Splits a Hugging Face dataset into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "    dataset (Dataset): The dataset to split.\n",
    "    test_size (float): The proportion of the dataset to include in the test split (between 0 and 1).\n",
    "    seed (int, optional): A seed for random shuffling for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Two datasets, the first being the training set and the second the testing set.\n",
    "    \"\"\"\n",
    "    # Shuffle the dataset\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        shuffled_indices = random.sample(range(len(dataset)), len(dataset))\n",
    "    else:\n",
    "        shuffled_indices = list(range(len(dataset)))\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(dataset) * (1 - test_size))\n",
    "\n",
    "    # Split the dataset\n",
    "    train_indices = shuffled_indices[:split_index]\n",
    "    test_indices = shuffled_indices[split_index:]\n",
    "\n",
    "    train_dataset = dataset.select(train_indices)\n",
    "    test_dataset = dataset.select(test_indices)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(preprocessed_dataset[\"train\"], test_size=0.2, seed=42)\n",
    "\n",
    "# Create a new DatasetDict with the new splits\n",
    "final_dataset = DatasetDict({\n",
    "    'train': train_set,\n",
    "    'test': test_set\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"mymodels/{modelname}-conference\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"all\",\n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally create the Trainer class and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_name = \"alitolga/bert-base-uncased-conference\"\n",
    "repo_name = \"alitolga/gpt2-conference\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(repo_name)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, repo_name, config=config)\n",
    "# model = PeftModel.from_pretrained(base_model, repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = pereira_dataset[\"train\"][\"sentences\"]\n",
    "sentences = sentences[0] # 0th subject\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the sentence embeddings from the Peft model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    \n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "    embeddings = torch.mean(hidden_states[0], dim=1)\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the Brain Decoding Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the voxels. For simplicity we start with all the brain regions\n",
    "fmri_data = pereira_dataset[\"train\"][\"all\"]\n",
    "\n",
    "# fMRI data of the first subject out of 8\n",
    "voxels = np.array(fmri_data[0])\n",
    "print(voxels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the embeddings\n",
    "# embeddings_normed = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare nested CV.\n",
    "# Inner CV is responsible for hyperparameter optimization;\n",
    "# Outer CV is responsible for prediction.\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "state = int(time.time())\n",
    "inner_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=state)\n",
    "\n",
    "# Final data prep: normalize.\n",
    "X = voxels - voxels.mean(axis=0)\n",
    "X = X / np.linalg.norm(X, axis=1, keepdims=True)\n",
    "Y = embeddings - embeddings.mean(axis=0)\n",
    "Y = Y / np.linalg.norm(Y, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Run learning.\n",
    "\n",
    "n_jobs = 4\n",
    "\n",
    "# Candidate ridge regression regularization parameters.\n",
    "ALPHAS = [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e1]\n",
    "\n",
    "# Run inner CV.\n",
    "gs = GridSearchCV(Ridge(fit_intercept=False),\n",
    "                {\"alpha\": ALPHAS}, cv=inner_cv, n_jobs=n_jobs, verbose=10)\n",
    "\n",
    "\"\"\"\n",
    "Purpose of This Line\n",
    "\n",
    "Nested Cross-Validation:\n",
    "\n",
    "The use of cross_val_predict with GridSearchCV (gs in this context) as the estimator \n",
    "is a part of a nested cross-validation strategy. \n",
    "The key purpose here is to evaluate the model's performance in a way that is as unbiased as possible.\n",
    "\n",
    "Independent Data Splits:\n",
    "\n",
    "The outer cross-validation (cv=outer_cv) splits the dataset into training and test sets multiple times \n",
    "(based on the number of folds in outer_cv). For each of these splits, \n",
    "the inner cross-validation (within GridSearchCV) finds the best alpha value. \n",
    "\n",
    "This process ensures that the choice of hyperparameters (alpha in this case) is not biased by the \n",
    "particular split of data used for model training and evaluation.\n",
    "\n",
    "Generating Unbiased Predictions:\n",
    "\n",
    "cross_val_predict does not simply fit the model but generates predictions for each point \n",
    "when it is in the test set of the outer cross-validation. \n",
    "These predictions are made by a model that has never seen the data point during training, \n",
    "thereby providing an unbiased estimate of the model's performance on unseen data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run outer CV.\n",
    "decoder_predictions = cross_val_predict(gs, X, Y, cv=outer_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decoder_predictions.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of Pairwise Accuracy Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, clone\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def pairwise_accuracy(\n",
    "    estimator: BaseEstimator = None,\n",
    "    X: torch.Tensor = None,  # noqa\n",
    "    y: torch.Tensor = None,\n",
    "    topic_ids: torch.Tensor = None,\n",
    "    scoring_variation: str = None,  # type: ignore\n",
    ") -> float:\n",
    "    \"\"\"Calculate the average pairwise accuracy of all pairs of true and predicted vectors.\n",
    "\n",
    "    Based on the pairwise accuracy as defined in Oota et al. 2022, Sun et al. 2021, Pereira et al. 2018.\n",
    "\n",
    "    :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "    :type estimator: BaseEstimator\n",
    "    :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "    :type X: torch.Tensor\n",
    "    :param y: True MRI vectors\n",
    "    :type y: torch.Tensor\n",
    "    :param topic_ids: Topic IDs for each paragraph\n",
    "    :type topic_ids: torch.Tensor\n",
    "    :param scoring_variation: Variation of the scoring function, defaults to None\n",
    "    :type scoring_variation: str\n",
    "    :return: Average pairwise accuracy from all possible sentence pairs\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    pred = estimator.predict(X)  # noqa\n",
    "\n",
    "    if scoring_variation == \"same-topic\":\n",
    "        # Calculate pairwise accuracy for same-topic sentences\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X)) if topic_ids[i] == topic_ids[j]\n",
    "        ]\n",
    "    elif scoring_variation == \"different-topic\":\n",
    "        # Calculate pairwise accuracy for different-topic sentences\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X)) if topic_ids[i] != topic_ids[j]\n",
    "        ]\n",
    "    else:\n",
    "        # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "        # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "        res = [\n",
    "            cosine(pred[i], y[i]) + cosine(pred[j], y[j]) < cosine(pred[i], y[j]) + cosine(pred[j], y[i])\n",
    "            for i in range(len(X))\n",
    "            for j in range(i + 1, len(X))\n",
    "        ]\n",
    "\n",
    "    # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "    return sum(res) / len(res)\n",
    "\n",
    "\n",
    "def pearson_scoring(\n",
    "    estimator: BaseEstimator = None,\n",
    "    X: torch.Tensor = None,  # noqa\n",
    "    y: torch.Tensor = None,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the average pearson correlation for the given set of true and predicted MRI vectors.\n",
    "\n",
    "    :param estimator: Estimator object (e.g., a Ridge regression)\n",
    "    :type estimator: BaseEstimator\n",
    "    :param X: Sentence embeddings used as a basis to predict MRI vectors with the estimator\n",
    "    :type X: torch.Tensor\n",
    "    :param y: True MRI vectors\n",
    "    :type y: torch.Tensor\n",
    "    :return: Average pearson correlation from all pairs of predicted and true MRI vectors\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    pred = estimator.predict(X)  # noqa\n",
    "\n",
    "    # See for all possible sentence pairings: Is the distance between the correct matches of predicted and X\n",
    "    # sentences smaller than the distance between pairings of X and predicted vectors from different sentences?\n",
    "    res = [pearsonr(t, p).statistic for t, p in zip(y, pred)]\n",
    "\n",
    "    # Return the fraction of instances for which the condition holds versus all possible pairs\n",
    "    return np.mean(res)  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Evaluate.\n",
    "\n",
    "Y_flatten = Y.flatten()\n",
    "pred_flatten = decoder_predictions.flatten()\n",
    "\n",
    "# Evaluate the performance (e.g., using mean squared error)\n",
    "mse = mean_squared_error(Y, decoder_predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "r2 = r2_score(Y, decoder_predictions)\n",
    "print(f\"R-squared (R2) Score: {r2}\")\n",
    "\n",
    "# Pearson Correlation Coefficient\n",
    "res = [pearsonr(t, p).statistic for t, p in zip(Y, decoder_predictions)]\n",
    "pearson_corr = np.mean(res)\n",
    "print(f\"Pearson Correlation Coefficient: {pearson_corr}\")\n",
    "\n",
    "# Cosine Similarity\n",
    "cosine_sim = np.mean(cosine_similarity(decoder_predictions, Y))\n",
    "print(f\"Cosine Similarity: {cosine_sim}\")\n",
    "\n",
    "# Pairwise Accuracy\n",
    "res = [ cosine(decoder_predictions[i], Y[i]) + cosine(decoder_predictions[j], Y[j]) < cosine(decoder_predictions[i], Y[j]) + cosine(decoder_predictions[j], Y[i])\n",
    "        for i in range(len(X))\n",
    "        for j in range(i + 1, len(X))\n",
    "    ]\n",
    "pairwise_acc = sum(res) / len(res)\n",
    "print(f\"Pairwise Accuracy: {pairwise_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
